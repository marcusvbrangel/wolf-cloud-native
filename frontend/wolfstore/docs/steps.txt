

------------------------------------------------------------------------------------------------------------------------




echo "# wolf-cloud-native" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/marcusvbrangel/wolf-cloud-native.git
git push -u origin main












1. criar o projeto angular

  ng new wolfstore


2. fazer o build do projeto

  ng build

    Initial chunk files   | Names         |  Raw size | Estimated transfer size
    main-KZ3DSSKO.js      | main          | 196.34 kB |                52.88 kB
    polyfills-FFHMD2TL.js | polyfills     |  34.52 kB |                11.28 kB
    styles-5INURTSO.css   | styles        |   0 bytes |                 0 bytes

                          | Initial total | 230.86 kB |                64.16 kB

    Application bundle generation complete. [5.568 seconds]

    Output location: ~/Documentos/desenvolvimento/estudos/wolf-cloud-native/frontend/wolfstore/dist/wolfstore


3. criar .dockerignore


4. criar o dockerfile



5. criar a imagem


  docker image build -t wolfstore:1.0 .


      [+] Building 6.9s (19/19) FINISHED                                          docker:default
       => [internal] load build definition from Dockerfile                                  0.0s
       => => transferring dockerfile: 487B                                                  0.0s
       => [internal] load metadata for docker.io/library/nginx:1.25.2-alpine                1.2s
       => [internal] load metadata for docker.io/library/node:hydrogen-alpine3.20           1.2s
       => [auth] library/nginx:pull token for registry-1.docker.io                          0.0s
       => [auth] library/node:pull token for registry-1.docker.io                           0.0s
       => [internal] load .dockerignore                                                     0.0s
       => => transferring context: 151B                                                     0.0s
       => [build 1/8] FROM docker.io/library/node:hydrogen-alpine3.20@sha256:a25c800a782c8  0.0s
       => [internal] load build context                                                     0.0s
       => => transferring context: 14.33kB                                                  0.0s
       => [stage-1 1/3] FROM docker.io/library/nginx:1.25.2-alpine@sha256:7272a6e0f728e95c  0.0s
       => CACHED [build 2/8] WORKDIR /app                                                   0.0s
       => CACHED [build 3/8] COPY package*.json .                                           0.0s
       => CACHED [build 4/8] RUN npm install                                                0.0s
       => CACHED [build 5/8] RUN #npx ngcc --properties es2023 browser module main --first  0.0s
       => [build 6/8] RUN npx ngcc --properties es2023 browser module main --first-only --  0.8s
       => [build 7/8] COPY . .                                                              0.1s
       => [build 8/8] RUN npm run build --prod                                              4.7s
       => CACHED [stage-1 2/3] COPY --from=build /app/dist/wolfstore/ /usr/share/nginx/htm  0.0s
       => CACHED [stage-1 3/3] COPY ./nginx/default.conf /etc/nginx/conf.d                  0.0s
       => exporting to image                                                                0.0s
       => => exporting layers                                                               0.0s
       => => writing image sha256:e7113a328e2ec8e63cb5bf6f6c3585d1ee76c87d1db57a027194bb79  0.0s
       => => naming to docker.io/library/wolfstore:1.0




  docker image ls

      REPOSITORY   TAG              IMAGE ID       CREATED              SIZE
      wolfstore    1.0              a3d227df927e   About a minute ago   42.9MB




  docker container run -p 8085:80 wolfstore:1.0

      /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
      /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
      10-listen-on-ipv6-by-default.sh: info: IPv6 listen already enabled
      /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
      /docker-entrypoint.sh: Configuration complete; ready for start up
      2025/03/29 04:59:01 [notice] 1#1: using the "epoll" event method
      2025/03/29 04:59:01 [notice] 1#1: nginx/1.25.2
      2025/03/29 04:59:01 [notice] 1#1: built by gcc 12.2.1 20220924 (Alpine 12.2.1_git20220924-r10)
      2025/03/29 04:59:01 [notice] 1#1: OS: Linux 6.11.0-19-generic
      2025/03/29 04:59:01 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
      2025/03/29 04:59:01 [notice] 1#1: start worker processes
      2025/03/29 04:59:01 [notice] 1#1: start worker process 22
      2025/03/29 04:59:01 [notice] 1#1: start worker process 23
      2025/03/29 04:59:01 [notice] 1#1: start worker process 24
      2025/03/29 04:59:01 [notice] 1#1: start worker process 25












apk add vim

vim /etc/nginx/conf.d/default.conf

server {
    listen 80;
    server_name localhost;

    location / {
        root /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }

    error_page 404 /index.html;
    location = /index.html {
        internal;
    }
}


 include /etc/nginx/conf.d/*.conf;

ls conf.d/
default.conf

cat /etc/nginx/conf.d/default.conf

vim /etc/nginx/conf.d/default.conf




------------------------------------------------------------------------------------------------------------------------



7. enviar imagem para o docker hub


https://hub.docker.com/

marcusvbrangel/wolfstore



docker tag wolfstore:1.0 marcusvbrangel/wolfstore:1.0



docker push marcusvbrangel/wolfstore:1.0


      The push refers to repository [docker.io/marcusvbrangel/wolfstore]
      43d130950a60: Pushed
      2a5453859db0: Pushed
      ae609780e57c: Mounted from library/nginx
      6f4a8e682735: Mounted from library/nginx
      f1e97ae5229e: Mounted from library/nginx
      f98384134eae: Mounted from library/nginx
      ea06dfa6cd92: Mounted from library/nginx
      feb2ab09a002: Mounted from library/nginx
      8fb91325d019: Mounted from library/nginx
      cc2447e1835a: Mounted from library/nginx
      1.0: digest: sha256:1900297cb3693a8bb57f6bd20c6dd2d14ba6567f9679113d2144f2c1a64c7cd5 size: 2406






------------------------------------------------------------------------------------------------------------------------





8. Configurar artefatos do Kubernetes


mkdir k8s

  frontend-deployment.yaml

  frontend-service.yaml

  frontend-configmap.yaml

  frontend-secrets.yaml




------------------------------------------------------------------------------------------------------------------------



docker --version

    Docker version 28.0.4, build b8034c0


kubectl version

    Client Version: v1.32.3
    Kustomize Version: v5.5.0


minikube version

    minikube version: v1.33.1





------------------------------------------------------------------------------------------------------------------------


minikube start --cpus=4 --memory=8192


- Criar um Cluster com um Perfil Personalizado
  Neste exemplo, "wolf-cluster" Ã© o nome do perfil e do cluster.

    minikube start -p wolf-cluster


- Listar Perfis no Minikube

    minikube profile list


- ConfiguraÃ§Ã£o de Contextos no Kubernetes

    kubectl config set-context my-custom-context --cluster=my-custom-cluster --user=my-user



- Alternar para um Contexto

      kubectl config use-context my-custom-context



- Listar todos os Contextos

      kubectl config get-contexts



Nota: Perfis do Minikube: Use perfis para criar e gerenciar mÃºltiplos clusters Minikube com nomes personalizados.
      Contextos do Kubernetes: Configure contextos para alternar facilmente entre diferentes clusters ou perfis no Kubernetes.




------------------------------------------------------------------------------------------------------------------------









------------------------------------------------------------------------------------------------------------------------


Para instalar a versÃ£o mais nova do Minikube, siga os passos abaixo:

Baixe o binÃ¡rio mais recente do Minikube:

    curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64


Torne o binÃ¡rio executÃ¡vel:

    chmod +x minikube-linux-amd64


Mova o binÃ¡rio para um diretÃ³rio incluÃ­do no seu PATH:

    sudo mv minikube-linux-amd64 /usr/local/bin/minikube


Verifique a versÃ£o instalada para confirmar a atualizaÃ§Ã£o:

    minikube version

          minikube version: v1.35.0


minikube status





------------------------------------------------------------------------------------------------------------------------




O erro indica que o contÃªiner do Minikube nÃ£o estÃ¡ em execuÃ§Ã£o ou nÃ£o foi encontrado. Para resolver isso, vocÃª pode tentar reiniciar o Minikube. Siga os passos abaixo:


Pare o Minikube, caso esteja em execuÃ§Ã£o:


minikube stop
Exclua qualquer instÃ¢ncia existente do Minikube:


minikube delete
Inicie o Minikube novamente:


minikube start
Verifique o status do Minikube:


minikube status
Esses comandos devem ajudar a resolver o problema e reiniciar o Minikube corretamente.









------------------------------------------------------------------------------------------------------------------------




minikube start


      ğŸ˜„  minikube v1.35.0 on Ubuntu 24.04
      âœ¨  Automatically selected the docker driver. Other choices: none, ssh
      ğŸ“Œ  Using Docker driver with root privileges
      ğŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ’¾  Downloading Kubernetes v1.32.0 preload ...
          > preloaded-images-k8s-v18-v1...:  333.57 MiB / 333.57 MiB  100.00% 5.41 Mi
          > gcr.io/k8s-minikube/kicbase...:  500.31 MiB / 500.31 MiB  100.00% 6.57 Mi
      ğŸ”¥  Creating docker container (CPUs=2, Memory=3900MB) ...
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª Generating certificates and keys ...
          â–ª Booting up control plane ...
          â–ª Configuring RBAC rules ...
      ğŸ”—  Configuring bridge CNI (Container Networking Interface) ...
      ğŸ”  Verifying Kubernetes components...
          â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
      ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
      ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default



minikube status


      minikube
      type: Control Plane
      host: Running
      kubelet: Running
      apiserver: Running
      kubeconfig: Configured




------------------------------------------------------------------------------------------------------------------------




Passos para Aplicar o Deployment


    kubectl apply -f k8s/frontend-deployment.yaml


    kubectl get deployments -o wide

        NAME                  READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                         SELECTOR
        frontend-deployment   3/3     3            3           86s   frontend     marcusvbrangel/wolfstore:1.0   app=frontend




    kubectl get pods -o wide

        NAME                                  READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
        frontend-deployment-b84cc7d45-65nls   1/1     Running   0          2m33s   10.244.0.3   minikube   <none>           <none>
        frontend-deployment-b84cc7d45-fccm9   1/1     Running   0          2m33s   10.244.0.5   minikube   <none>           <none>
        frontend-deployment-b84cc7d45-tzqq6   1/1     Running   0          2m33s   10.244.0.4   minikube   <none>           <none>






------------------------------------------------------------------------------------------------------------------------



ConfiguraÃ§Ãµes do ServiÃ§o



kubectl apply -f k8s/frontend-service.yaml




kubectl get services -o wide

    NAME               TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
    frontend-service   LoadBalancer   10.96.254.101   <pending>     8090:32699/TCP   39s   app=frontend






------------------------------------------------------------------------------------------------------------------------



Usar o Minikube Tunnel

O Minikube fornece uma ferramenta chamada minikube tunnel que pode ser usada para expor serviÃ§os do tipo LoadBalancer.


    minikube tunnel


        [sudo] senha para wolf:
        Status:
                machine: minikube
                pid: 320451
                route: 10.96.0.0/12 -> 192.168.49.2
                minikube: Running
                services: [frontend-service]
            errors:
                        minikube: no errors
                        router: no errors
                        loadbalancer emulator: no errors




kubectl get services -o wide


      NAME               TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE     SELECTOR
      frontend-service   LoadBalancer   10.96.254.101   10.96.254.101   8090:32699/TCP   7m54s   app=frontend





http://10.96.254.101:8090/






curl -i 10.96.254.101:8090


      HTTP/1.1 200 OK
      Server: nginx/1.25.2
      Date: Sat, 29 Mar 2025 06:42:55 GMT
      Content-Type: text/html
      Content-Length: 482
      Last-Modified: Sat, 29 Mar 2025 02:13:49 GMT
      Connection: keep-alive
      ETag: "67e7575d-1e2"
      Accept-Ranges: bytes

      <!doctype html>
      <html lang="en" data-beasties-container>
      <head>
        <meta charset="utf-8">
        <title>Wolfstore</title>
        <base href="/">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" type="image/x-icon" href="favicon.ico">
      <link rel="stylesheet" href="styles-5INURTSO.css"></head>
      <body>
        <app-root></app-root>
      <script src="polyfills-FFHMD2TL.js" type="module"></script><script src="main-KZ3DSSKO.js" type="module"></script></body>





------------------------------------------------------------------------------------------------------------------------







minikube start -p wolf-cluster --nodes=3


      ğŸ˜„  [wolf-cluster] minikube v1.35.0 on Ubuntu 24.04
      âœ¨  Automatically selected the docker driver
      ğŸ“Œ  Using Docker driver with root privileges

      ğŸ‘  Starting "wolf-cluster" primary control-plane node in "wolf-cluster" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª Generating certificates and keys ...
          â–ª Booting up control plane ...
          â–ª Configuring RBAC rules ...
      ğŸ”—  Configuring CNI (Container Networking Interface) ...
      ğŸ”  Verifying Kubernetes components...
          â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
      ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass

      ğŸ‘  Starting "wolf-cluster-m02" worker node in "wolf-cluster" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
      ğŸŒ  Found network options:
          â–ª NO_PROXY=192.168.49.2
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª env NO_PROXY=192.168.49.2
      ğŸ”  Verifying Kubernetes components...

      ğŸ‘  Starting "wolf-cluster-m03" worker node in "wolf-cluster" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
      ğŸŒ  Found network options:
          â–ª NO_PROXY=192.168.49.2,192.168.49.3
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª env NO_PROXY=192.168.49.2
          â–ª env NO_PROXY=192.168.49.2,192.168.49.3
      ğŸ”  Verifying Kubernetes components...
      ğŸ„  Done! kubectl is now configured to use "wolf-cluster" cluster and "default" namespace by default











------------------------------------------------------------------------------------------------------------------------


1. Verificar o Status do Cluster
sh
minikube status -p wolf-cluster
2. Listar Todos os NÃ³s no Cluster
sh
kubectl get nodes --context=wolf-cluster
3. Obter InformaÃ§Ãµes Detalhadas de um NÃ³
sh
kubectl describe node <node-name>
4. Verificar os Pods em Todos os Namespaces
sh
kubectl get pods --all-namespaces
5. Listar Todos os ServiÃ§os em Todos os Namespaces
sh
kubectl get services --all-namespaces
6. Obter InformaÃ§Ãµes Sobre o Contexto Atual
sh
kubectl config current-context
7. Obter InformaÃ§Ãµes Sobre o Cluster
sh
kubectl cluster-info
8. Obter InformaÃ§Ãµes Detalhadas de um Pod
sh
kubectl describe pod <pod-name> -n <namespace>
9. Verificar o Uso de Recursos de um NÃ³
sh
kubectl top node
10. Verificar o Uso de Recursos de um Pod
sh
kubectl top pod -n <namespace>
Exemplos
Verificar o Status do Cluster:

sh
minikube status -p wolf-cluster
Listar Todos os NÃ³s no Cluster:

sh
kubectl get nodes --context=minikube
Obter InformaÃ§Ãµes Detalhadas de um NÃ³:

sh
kubectl describe node minikube
Verificar os Pods em Todos os Namespaces:

sh
kubectl get pods --all-namespaces
Listar Todos os ServiÃ§os em Todos os Namespaces:

sh
kubectl get services --all-namespaces
Obter InformaÃ§Ãµes Sobre o Contexto Atual:

sh
kubectl config current-context
Obter InformaÃ§Ãµes Sobre o Cluster:

sh
kubectl cluster-info
Obter InformaÃ§Ãµes Detalhadas de um Pod:

sh
kubectl describe pod my-pod -n default
Verificar o Uso de Recursos de um NÃ³:

sh
kubectl top node
Verificar o Uso de Recursos de um Pod:

sh
kubectl top pod -n default


------------------------------------------------------------------------------------------------------------------------


kubectl apply -f k8s/namespace.yaml


kubectl apply -f k8s/

      configmap/frontend-config created
      deployment.apps/frontend-deployment created
      secret/frontend-secrets created
      service/frontend-service created
      namespace/wolfns unchanged



kubectl config set-context --current --namespace=wolfns



------------------------------------------------------------------------------------------------------------------------




minikube tunnel -p wolf-cluster


        [sudo] senha para wolf:
        Status:
          machine: wolf-cluster
          pid: 988097
          route: 10.96.0.0/12 -> 192.168.49.2
          minikube: Running
          services: [frontend-service, frontend-service]
            errors:
            minikube: no errors
            router: no errors
            loadbalancer emulator: no errors




kubectl get svc


        NAME               TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
        frontend-service   LoadBalancer   10.111.62.235   10.111.62.235   8090:30124/TCP   7m53s




------------------------------------------------------------------------------------------------------------------------



Instalar o Helm


# Instalar o Helm


curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash


          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 11913  100 11913    0     0  36471      0 --:--:-- --:--:-- --:--:-- 36542
        Downloading https://get.helm.sh/helm-v3.17.2-linux-amd64.tar.gz
        Verifying checksum... Done.
        Preparing to install helm into /usr/local/bin
        [sudo] senha para wolf:
        helm installed into /usr/local/bin/helm



helm version

        version.BuildInfo{Version:"v3.17.2", GitCommit:"cc0bbbd6d6276b83880042c1ecb34087e84d41eb", GitTreeState:"clean", GoVersion:"go1.23.7"}



------------------------------------------------------------------------------------------------------------------------



Instalar o cert-manager


Vamos instalar o cert-manager para gerenciar certificados TLS com Let's Encrypt.


# Adicionar o repositÃ³rio Helm do cert-manager

helm repo add jetstack https://charts.jetstack.io

helm repo update

      Hang tight while we grab the latest from your chart repositories...
      ...Successfully got an update from the "jetstack" chart repository
      ...Successfully got an update from the "kubernetes-dashboard" chart repository
      ...Successfully got an update from the "prometheus-community" chart repository
      ...Successfully got an update from the "bitnami" chart repository



# Criar o namespace para o cert-manager

kubectl create namespace cert-manager



# Instalar o cert-manager usando o Helm

helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.5.3



# Verificar a instalaÃ§Ã£o do cert-manager

kubectl get pods --namespace cert-manager


      NAME                                       READY   STATUS    RESTARTS      AGE
      cert-manager-78764f8796-bjfmm              1/1     Running   0             4m25s
      cert-manager-cainjector-7dd7ccf897-srqwh   1/1     Running   0             4m25s
      cert-manager-startupapicheck-szszg         1/1     Running   3 (49s ago)   4m24s
      cert-manager-webhook-68fddb647b-4zj5t      1/1     Running   0             4m25s





------------------------------------------------------------------------------------------------------------------------



Configurar o Issuer ou ClusterIssuer


Crie um arquivo letsencrypt-issuer.yaml para configurar o Issuer ou ClusterIssuer:


../wolf-cloud-native/cluster-kubernetes/letsencrypt-issuer.yaml


apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: your-email@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx



kubectl apply -f letsencrypt-issuer.yaml


    kubectl apply -f letsencrypt-issuer.yaml
    error: resource mapping not found for name: "letsencrypt-prod" namespace: "" from "letsencrypt-issuer.yaml": no matches for kind "ClusterIssuer" in version "cert-manager.io/v1"
    ensure CRDs are installed first



kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.crds.yaml


      customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created




kubectl get crds | grep cert-manager


      certificaterequests.cert-manager.io   2025-03-30T04:46:18Z
      certificates.cert-manager.io          2025-03-30T04:46:18Z
      challenges.acme.cert-manager.io       2025-03-30T04:46:18Z
      clusterissuers.cert-manager.io        2025-03-30T04:46:18Z
      issuers.cert-manager.io               2025-03-30T04:46:18Z
      orders.acme.cert-manager.io           2025-03-30T04:46:19Z




kubectl apply -f letsencrypt-issuer.yaml


      clusterissuer.cert-manager.io/letsencrypt-prod created








------------------------------------------------------------------------------------------------------------------------



5. Instalar o NGINX Kubernetes Gateway


# Adicionar o repositÃ³rio Helm do NGINX

helm repo add nginx-stable https://helm.nginx.com/stable

      "nginx-stable" has been added to your repositories



helm repo update

      Hang tight while we grab the latest from your chart repositories...
      ...Successfully got an update from the "nginx-stable" chart repository
      ...Successfully got an update from the "kubernetes-dashboard" chart repository
      ...Successfully got an update from the "jetstack" chart repository
      ...Successfully got an update from the "prometheus-community" chart repository
      ...Successfully got an update from the "bitnami" chart repository
      Update Complete. âˆHappy Helming!âˆ



# Instalar o NGINX Kubernetes Gateway

helm install nginx-gateway nginx-stable/nginx-kubernetes-gateway

      helm repo update
      Hang tight while we grab the latest from your chart repositories...
      ...Successfully got an update from the "kubernetes-dashboard" chart repository
      ...Successfully got an update from the "nginx-stable" chart repository
      ...Successfully got an update from the "jetstack" chart repository
      ...Successfully got an update from the "prometheus-community" chart repository
      ...Successfully got an update from the "bitnami" chart repository
      Update Complete. âˆHappy Helming!âˆ
      wolf@dell-Inspiron:$ helm install nginx-gateway nginx-stable/nginx-kubernetes-gateway
      Error: INSTALLATION FAILED: chart "nginx-kubernetes-gateway" matching  not found in nginx-stable index. (try 'helm repo update'): no chart name found

      O erro indica que o grÃ¡fico nginx-kubernetes-gateway nÃ£o foi encontrado no repositÃ³rio nginx-stable. Isso pode ocorrer se o nome do grÃ¡fico estiver incorreto ou se ele nÃ£o estiver disponÃ­vel no repositÃ³rio.






helm search repo nginx-stable

      NAME                                        	CHART VERSION	APP VERSION	DESCRIPTION
      nginx-stable/nginx-appprotect-dos-arbitrator	0.1.0        	1.1.0      	NGINX App Protect Dos arbitrator
      nginx-stable/nginx-devportal                	1.7.2        	1.7.2      	A Helm chart for deploying ACM Developer Portal
      nginx-stable/nginx-ingress                  	2.0.1        	4.0.1      	NGINX Ingress Controller
      nginx-stable/nginx-service-mesh             	2.0.0        	           	NGINX Service Mesh
      nginx-stable/nms                            	1.16.0       	NIM 2.19.0 	A chart for installing the NGINX Management Suite
      nginx-stable/nms-acm                        	1.9.3        	1.9.3      	A Helm chart for Kubernetes
      nginx-stable/nms-adm                        	4.0.0        	4.0.0      	A Helm chart for ADM
      nginx-stable/nms-hybrid                     	2.19.1       	2.19.1     	A Helm chart for Kubernetes



helm install nginx-gateway nginx-stable/nginx-ingress


      NAME: nginx-gateway
      LAST DEPLOYED: Sun Mar 30 01:55:56 2025
      NAMESPACE: wolfns
      STATUS: deployed
      REVISION: 1
      TEST SUITE: None
      NOTES:
      NGINX Ingress Controller 4.0.1 has been installed.

      For release notes for this version please see: https://docs.nginx.com/nginx-ingress-controller/releases/

      Installation and upgrade instructions: https://docs.nginx.com/nginx-ingress-controller/installation/installing-nic/installation-with-helm/








------------------------------------------------------------------------------------------------------------------------



6. Configurar o Ingress para Usar o Certificado


Crie um arquivo ingress.yaml para configurar o Ingress para usar o certificado Let's Encrypt:




apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"  # Use "issuer" se vocÃª estiver usando Issuer
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - wolfstore.com
    secretName: example-tls
  rules:
  - host: wolfstore.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-service
            port:
              number: 80



kubectl apply -f ingress.yaml

    ingress.networking.k8s.io/example-ingress created











------------------------------------------------------------------------------------------------------------------------




7. Configurar Network Policies


Crie um arquivo network-policy.yaml para definir regras de rede:



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: backend
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          role: backend
    ports:
    - protocol: TCP
      port: 80






kubectl apply -f network-policy.yaml




------------------------------------------------------------------------------------------------------------------------




Configurar RBAC


Crie um arquivo rbac.yaml para gerenciar permissÃµes:





apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: "jane"
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io




kubectl apply -f rbac.yaml

    role.rbac.authorization.k8s.io/pod-reader created
    rolebinding.rbac.authorization.k8s.io/read-pods created





------------------------------------------------------------------------------------------------------------------------



9. Configurar Pod Security Policies


Crie um arquivo pod-security-policy.yaml para aplicar restriÃ§Ãµes aos pods:




apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  allowedCapabilities: []
  volumes:
  - 'configMap'
  - 'emptyDir'
  - 'projected'
  - 'secret'
  - 'downwardAPI'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535




kubectl apply -f pod-security-policy.yaml


    error: resource mapping not found for name: "restricted-psp" namespace: "" from "pod-security-policy.yaml": no matches for kind "PodSecurityPolicy" in version "policy/v1"
    ensure CRDs are installed first
    wolf@dell-Inspiron:$





------------------------------------------------------------------------------------------------------------------------



docker image build -t wolfstore-frontend .


docker container run -p 8085:80 wolfstore-frontend


docker tag wolfstore-frontend marcusvbrangel/wolfstore-frontend:1.1.2


docker push marcusvbrangel/wolfstore-frontend:1.1.2




------------------------------------------------------------------------------------------------------------------------


kubectl apply -f k8s/wolfstore-backend-ingress.yaml

kubectl get ingress

kubectl describe ingress wolfstore-backend-ingress


------------------------------------------------------------------------------------------------------------------------


POSTGRES


helm repo add bitnami https://charts.bitnami.com/bitnami

helm repo update

helm install my-postgresql bitnami/postgresql

helm list

kubectl get pods




------------------------------------------------------------------------------------------------------------------------


NAME: my-postgresql
LAST DEPLOYED: Tue Apr  1 00:29:22 2025
NAMESPACE: wolfns
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: postgresql
CHART VERSION: 16.6.0
APP VERSION: 17.4.0

Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami for more information.

** Please be patient while the chart is being deployed **

PostgreSQL can be accessed via port 5432 on the following DNS names from within your cluster:

    my-postgresql.wolfns.svc.cluster.local - Read/Write connection

To get the password for "postgres" run:

    export POSTGRES_PASSWORD=$(kubectl get secret --namespace wolfns my-postgresql -o jsonpath="{.data.postgres-password}" | base64 -d)

To connect to your database run the following command:

    kubectl run my-postgresql-client --rm --tty -i --restart='Never' --namespace wolfns --image docker.io/bitnami/postgresql:17.4.0-debian-12-r11 --env="PGPASSWORD=$POSTGRES_PASSWORD" \
      --command -- psql --host my-postgresql -U postgres -d postgres -p 5432

    > NOTE: If you access the container using bash, make sure that you execute "/opt/bitnami/scripts/postgresql/entrypoint.sh /bin/bash" in order to avoid the error "psql: local user with ID 1001} does not exist"

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace wolfns svc/my-postgresql 5432:5432 &
    PGPASSWORD="$POSTGRES_PASSWORD" psql --host 127.0.0.1 -U postgres -d postgres -p 5432

WARNING: The configured password will be ignored on new installation in case when previous PostgreSQL release was deleted through the helm command. In that case, old PVC will have an old password, and setting it through helm won't take effect. Deleting persistent volumes (PVs) will solve the issue.

WARNING: There are "resources" sections in the chart not set. Using "resourcesPreset" is not recommended for production. For production installations, please set the following values according to your workload needs:
  - primary.resources
  - readReplicas.resources
+info https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/




========================================================================================================================
========================================================================================================================
========================================================================================================================



minikube delete -p wolf-cluster


minikube start -p base-cluster --nodes=3 --cpus=2 --memory=2200


      ğŸ˜„  [base-cluster] minikube v1.35.0 on Ubuntu 24.04
      âœ¨  Automatically selected the docker driver
      ğŸ“Œ  Using Docker driver with root privileges

      ğŸ‘  Starting "base-cluster" primary control-plane node in "base-cluster" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª Generating certificates and keys ...
          â–ª Booting up control plane ...
          â–ª Configuring RBAC rules ...
      ğŸ”—  Configuring CNI (Container Networking Interface) ...
      ğŸ”  Verifying Kubernetes components...
          â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
      ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass

      ğŸ‘  Starting "base-cluster-m02" worker node in "base-cluster" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
      ğŸŒ  Found network options:
          â–ª NO_PROXY=192.168.58.2
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª env NO_PROXY=192.168.58.2
      ğŸ”  Verifying Kubernetes components...

      ğŸ‘  Starting "base-cluster-m03" worker node in "base-cluster" cluster
      ğŸšœ  Pulling base image v0.0.46 ...
      ğŸ”¥  Creating docker container (CPUs=2, Memory=2200MB) ...
      ğŸŒ  Found network options:
          â–ª NO_PROXY=192.168.58.2,192.168.58.3
      ğŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          â–ª env NO_PROXY=192.168.58.2
          â–ª env NO_PROXY=192.168.58.2,192.168.58.3
      ğŸ”  Verifying Kubernetes components...

      ğŸ„  Done! kubectl is now configured to use "base-cluster" cluster and "default" namespace by default




k get nodes -o wide


      NAME               STATUS   ROLES           AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
      base-cluster       Ready    control-plane   8m5s    v1.32.0   192.168.58.2   <none>        Ubuntu 22.04.5 LTS   6.11.0-21-generic   docker://27.4.1
      base-cluster-m02   Ready    <none>          7m45s   v1.32.0   192.168.58.3   <none>        Ubuntu 22.04.5 LTS   6.11.0-21-generic   docker://27.4.1
      base-cluster-m03   Ready    <none>          7m29s   v1.32.0   192.168.58.4   <none>        Ubuntu 22.04.5 LTS   6.11.0-21-generic   docker://27.4.1





minikube profile list


      |--------------|-----------|---------|--------------|------|---------|--------|-------|----------------|--------------------|
      |   Profile    | VM Driver | Runtime |      IP      | Port | Version | Status | Nodes | Active Profile | Active Kubecontext |
      |--------------|-----------|---------|--------------|------|---------|--------|-------|----------------|--------------------|
      | base-cluster | docker    | docker  | 192.168.58.2 | 8443 | v1.32.0 | OK     |     3 |                | *                  |
      |--------------|-----------|---------|--------------|------|---------|--------|-------|----------------|--------------------|



minikube profile base-cluster

      âœ…  minikube profile was successfully set to base-cluster




minikube profile list


      |--------------|-----------|---------|--------------|------|---------|--------|-------|----------------|--------------------|
      |   Profile    | VM Driver | Runtime |      IP      | Port | Version | Status | Nodes | Active Profile | Active Kubecontext |
      |--------------|-----------|---------|--------------|------|---------|--------|-------|----------------|--------------------|
      | base-cluster | docker    | docker  | 192.168.58.2 | 8443 | v1.32.0 | OK     |     3 | *              | *                  |
      |--------------|-----------|---------|--------------|------|---------|--------|-------|----------------|--------------------|





minikube status base-cluster

      base-cluster
      type: Control Plane
      host: Running
      kubelet: Running
      apiserver: Running
      kubeconfig: Configured

      base-cluster-m02
      type: Worker
      host: Running
      kubelet: Running

      base-cluster-m03
      type: Worker
      host: Running
      kubelet: Running




kubectl get namespaces


      NAME              STATUS   AGE
      default           Active   13m
      kube-node-lease   Active   13m
      kube-public       Active   13m
      kube-system       Active   13m




kubectl create namespace wolfns



kubectl config set-context --current --namespace=wolfns




------------------------------------------------------------------------------------------------------------------------



kubectl apply -f k8s/frontend-deployment.yaml

k get deploy -o wide

      NAME                            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS           IMAGES                                    SELECTOR
      wolfstore-frontend-deployment   2/2     2            2           59s   wolfstore-frontend   marcusvbrangel/wolfstore-frontend:1.1.1   app=wolfstore-frontend,technology=angular,type=frontend




k get pods -o wide

    wolfstore-frontend-deployment-5fccfb89d6-2gpdp   1/1     Running   0          4m44s   10.244.2.3   base-cluster-m03   <none>           <none>
    wolfstore-frontend-deployment-5fccfb89d6-tzzxj   1/1     Running   0          4m44s   10.244.1.3   base-cluster-m02   <none>           <none>



kubectl logs wolfstore-frontend-deployment-5fccfb89d6-2gpdp -n wolfns

      /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
      /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
      10-listen-on-ipv6-by-default.sh: info: IPv6 listen already enabled
      /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
      /docker-entrypoint.sh: Configuration complete; ready for start up
      2025/04/03 03:29:21 [notice] 1#1: using the "epoll" event method
      2025/04/03 03:29:21 [notice] 1#1: nginx/1.25.2
      2025/04/03 03:29:21 [notice] 1#1: built by gcc 12.2.1 20220924 (Alpine 12.2.1_git20220924-r10)
      2025/04/03 03:29:21 [notice] 1#1: OS: Linux 6.11.0-21-generic
      2025/04/03 03:29:21 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
      2025/04/03 03:29:21 [notice] 1#1: start worker processes
      2025/04/03 03:29:21 [notice] 1#1: start worker process 22
      2025/04/03 03:29:21 [notice] 1#1: start worker process 23
      2025/04/03 03:29:21 [notice] 1#1: start worker process 24
      2025/04/03 03:29:21 [notice] 1#1: start worker process 25




kubectl apply -f k8s/frontend-service.yaml

k get svc -o wide

      NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE     SELECTOR
      wolfstore-frontend-service   LoadBalancer   10.101.158.110   <pending>     8090:30340/TCP   2m41s   app=wolfstore-frontend,technology=angular,type=frontend




minikube tunnel -p base-cluster

      [sudo] senha para wolf:
      Status:
        machine: base-cluster
        pid: 1567298
        route: 10.96.0.0/12 -> 192.168.58.2
        minikube: Running
        services: [wolfstore-frontend-service]
          errors:
          minikube: no errors
          router: no errors
          loadbalancer emulator: no errors




TEST:

    <CLUSTER-IP><port>

    http://10.101.158.110:8090/

    curl -v http://10.101.158.110:8090/




------------------------------------------------------------------------------------------------------------------------



INGRESS



minikube addons list | grep ingress

      | ingress                     | base-cluster | disabled     | Kubernetes                     |
      | ingress-dns                 | base-cluster | disabled     | minikube



minikube ip -p base-cluster

      192.168.58.2




sudo vim /etc/hosts


cat /etc/hosts

      127.0.0.1 localhost
      127.0.1.1 dell-Inspiron

      # The following lines are desirable for IPv6 capable hosts
      ::1     ip6-localhost ip6-loopback
      fe00::0 ip6-localnet
      ff00::0 ip6-mcastprefix
      ff02::1 ip6-allnodes
      ff02::2 ip6-allrouters
      # Added by Docker Desktop
      # To allow the same kube context to work on the host and the container:
      127.0.0.1	kubernetes.docker.internal
      # End of section

      # My hosts definition
      192.168.58.2        wolfstore.com

















TIP:

kubectl api-resources | grep -i ingress

      ingressclasses                                   networking.k8s.io/v1              false        IngressClass
      ingresses                           ing          networking.k8s.io/v1              true         Ingress




------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------






