

------------------------------------------------------------------------------------------------------------------------




echo "# wolf-cloud-native" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/marcusvbrangel/wolf-cloud-native.git
git push -u origin main












1. criar o projeto angular

  ng new wolfstore


2. fazer o build do projeto

  ng build

    Initial chunk files   | Names         |  Raw size | Estimated transfer size
    main-KZ3DSSKO.js      | main          | 196.34 kB |                52.88 kB
    polyfills-FFHMD2TL.js | polyfills     |  34.52 kB |                11.28 kB
    styles-5INURTSO.css   | styles        |   0 bytes |                 0 bytes

                          | Initial total | 230.86 kB |                64.16 kB

    Application bundle generation complete. [5.568 seconds]

    Output location: ~/Documentos/desenvolvimento/estudos/wolf-cloud-native/frontend/wolfstore/dist/wolfstore


3. criar .dockerignore


4. criar o dockerfile



5. criar a imagem


  docker image build -t wolfstore:1.0 .


      [+] Building 6.9s (19/19) FINISHED                                          docker:default
       => [internal] load build definition from Dockerfile                                  0.0s
       => => transferring dockerfile: 487B                                                  0.0s
       => [internal] load metadata for docker.io/library/nginx:1.25.2-alpine                1.2s
       => [internal] load metadata for docker.io/library/node:hydrogen-alpine3.20           1.2s
       => [auth] library/nginx:pull token for registry-1.docker.io                          0.0s
       => [auth] library/node:pull token for registry-1.docker.io                           0.0s
       => [internal] load .dockerignore                                                     0.0s
       => => transferring context: 151B                                                     0.0s
       => [build 1/8] FROM docker.io/library/node:hydrogen-alpine3.20@sha256:a25c800a782c8  0.0s
       => [internal] load build context                                                     0.0s
       => => transferring context: 14.33kB                                                  0.0s
       => [stage-1 1/3] FROM docker.io/library/nginx:1.25.2-alpine@sha256:7272a6e0f728e95c  0.0s
       => CACHED [build 2/8] WORKDIR /app                                                   0.0s
       => CACHED [build 3/8] COPY package*.json .                                           0.0s
       => CACHED [build 4/8] RUN npm install                                                0.0s
       => CACHED [build 5/8] RUN #npx ngcc --properties es2023 browser module main --first  0.0s
       => [build 6/8] RUN npx ngcc --properties es2023 browser module main --first-only --  0.8s
       => [build 7/8] COPY . .                                                              0.1s
       => [build 8/8] RUN npm run build --prod                                              4.7s
       => CACHED [stage-1 2/3] COPY --from=build /app/dist/wolfstore/ /usr/share/nginx/htm  0.0s
       => CACHED [stage-1 3/3] COPY ./nginx/default.conf /etc/nginx/conf.d                  0.0s
       => exporting to image                                                                0.0s
       => => exporting layers                                                               0.0s
       => => writing image sha256:e7113a328e2ec8e63cb5bf6f6c3585d1ee76c87d1db57a027194bb79  0.0s
       => => naming to docker.io/library/wolfstore:1.0




  docker image ls

      REPOSITORY   TAG              IMAGE ID       CREATED              SIZE
      wolfstore    1.0              a3d227df927e   About a minute ago   42.9MB




  docker container run -p 8085:80 wolfstore:1.0

      /docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
      /docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
      10-listen-on-ipv6-by-default.sh: info: IPv6 listen already enabled
      /docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
      /docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
      /docker-entrypoint.sh: Configuration complete; ready for start up
      2025/03/29 04:59:01 [notice] 1#1: using the "epoll" event method
      2025/03/29 04:59:01 [notice] 1#1: nginx/1.25.2
      2025/03/29 04:59:01 [notice] 1#1: built by gcc 12.2.1 20220924 (Alpine 12.2.1_git20220924-r10)
      2025/03/29 04:59:01 [notice] 1#1: OS: Linux 6.11.0-19-generic
      2025/03/29 04:59:01 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
      2025/03/29 04:59:01 [notice] 1#1: start worker processes
      2025/03/29 04:59:01 [notice] 1#1: start worker process 22
      2025/03/29 04:59:01 [notice] 1#1: start worker process 23
      2025/03/29 04:59:01 [notice] 1#1: start worker process 24
      2025/03/29 04:59:01 [notice] 1#1: start worker process 25












apk add vim

vim /etc/nginx/conf.d/default.conf

server {
    listen 80;
    server_name localhost;

    location / {
        root /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }

    error_page 404 /index.html;
    location = /index.html {
        internal;
    }
}


 include /etc/nginx/conf.d/*.conf;

ls conf.d/
default.conf

cat /etc/nginx/conf.d/default.conf

vim /etc/nginx/conf.d/default.conf




------------------------------------------------------------------------------------------------------------------------



7. enviar imagem para o docker hub


https://hub.docker.com/

marcusvbrangel/wolfstore



docker tag wolfstore:1.0 marcusvbrangel/wolfstore:1.0



docker push marcusvbrangel/wolfstore:1.0


      The push refers to repository [docker.io/marcusvbrangel/wolfstore]
      43d130950a60: Pushed
      2a5453859db0: Pushed
      ae609780e57c: Mounted from library/nginx
      6f4a8e682735: Mounted from library/nginx
      f1e97ae5229e: Mounted from library/nginx
      f98384134eae: Mounted from library/nginx
      ea06dfa6cd92: Mounted from library/nginx
      feb2ab09a002: Mounted from library/nginx
      8fb91325d019: Mounted from library/nginx
      cc2447e1835a: Mounted from library/nginx
      1.0: digest: sha256:1900297cb3693a8bb57f6bd20c6dd2d14ba6567f9679113d2144f2c1a64c7cd5 size: 2406






------------------------------------------------------------------------------------------------------------------------





8. Configurar artefatos do Kubernetes


mkdir k8s

  frontend-deployment.yaml

  frontend-service.yaml

  frontend-configmap.yaml

  frontend-secrets.yaml




------------------------------------------------------------------------------------------------------------------------



docker --version

    Docker version 28.0.4, build b8034c0


kubectl version

    Client Version: v1.32.3
    Kustomize Version: v5.5.0


minikube version

    minikube version: v1.33.1





------------------------------------------------------------------------------------------------------------------------


minikube start --cpus=4 --memory=8192


- Criar um Cluster com um Perfil Personalizado
  Neste exemplo, "wolf-cluster" √© o nome do perfil e do cluster.

    minikube start -p wolf-cluster


- Listar Perfis no Minikube

    minikube profile list


- Configura√ß√£o de Contextos no Kubernetes

    kubectl config set-context my-custom-context --cluster=my-custom-cluster --user=my-user



- Alternar para um Contexto

      kubectl config use-context my-custom-context



- Listar todos os Contextos

      kubectl config get-contexts



Nota: Perfis do Minikube: Use perfis para criar e gerenciar m√∫ltiplos clusters Minikube com nomes personalizados.
      Contextos do Kubernetes: Configure contextos para alternar facilmente entre diferentes clusters ou perfis no Kubernetes.




------------------------------------------------------------------------------------------------------------------------









------------------------------------------------------------------------------------------------------------------------


Para instalar a vers√£o mais nova do Minikube, siga os passos abaixo:

Baixe o bin√°rio mais recente do Minikube:

    curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64


Torne o bin√°rio execut√°vel:

    chmod +x minikube-linux-amd64


Mova o bin√°rio para um diret√≥rio inclu√≠do no seu PATH:

    sudo mv minikube-linux-amd64 /usr/local/bin/minikube


Verifique a vers√£o instalada para confirmar a atualiza√ß√£o:

    minikube version

          minikube version: v1.35.0


minikube status





------------------------------------------------------------------------------------------------------------------------




O erro indica que o cont√™iner do Minikube n√£o est√° em execu√ß√£o ou n√£o foi encontrado. Para resolver isso, voc√™ pode tentar reiniciar o Minikube. Siga os passos abaixo:


Pare o Minikube, caso esteja em execu√ß√£o:


minikube stop
Exclua qualquer inst√¢ncia existente do Minikube:


minikube delete
Inicie o Minikube novamente:


minikube start
Verifique o status do Minikube:


minikube status
Esses comandos devem ajudar a resolver o problema e reiniciar o Minikube corretamente.









------------------------------------------------------------------------------------------------------------------------




minikube start


      üòÑ  minikube v1.35.0 on Ubuntu 24.04
      ‚ú®  Automatically selected the docker driver. Other choices: none, ssh
      üìå  Using Docker driver with root privileges
      üëç  Starting "minikube" primary control-plane node in "minikube" cluster
      üöú  Pulling base image v0.0.46 ...
      üíæ  Downloading Kubernetes v1.32.0 preload ...
          > preloaded-images-k8s-v18-v1...:  333.57 MiB / 333.57 MiB  100.00% 5.41 Mi
          > gcr.io/k8s-minikube/kicbase...:  500.31 MiB / 500.31 MiB  100.00% 6.57 Mi
      üî•  Creating docker container (CPUs=2, Memory=3900MB) ...
      üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          ‚ñ™ Generating certificates and keys ...
          ‚ñ™ Booting up control plane ...
          ‚ñ™ Configuring RBAC rules ...
      üîó  Configuring bridge CNI (Container Networking Interface) ...
      üîé  Verifying Kubernetes components...
          ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
      üåü  Enabled addons: storage-provisioner, default-storageclass
      üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default



minikube status


      minikube
      type: Control Plane
      host: Running
      kubelet: Running
      apiserver: Running
      kubeconfig: Configured




------------------------------------------------------------------------------------------------------------------------




Passos para Aplicar o Deployment


    kubectl apply -f k8s/frontend-deployment.yaml


    kubectl get deployments -o wide

        NAME                  READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                         SELECTOR
        frontend-deployment   3/3     3            3           86s   frontend     marcusvbrangel/wolfstore:1.0   app=frontend




    kubectl get pods -o wide

        NAME                                  READY   STATUS    RESTARTS   AGE     IP           NODE       NOMINATED NODE   READINESS GATES
        frontend-deployment-b84cc7d45-65nls   1/1     Running   0          2m33s   10.244.0.3   minikube   <none>           <none>
        frontend-deployment-b84cc7d45-fccm9   1/1     Running   0          2m33s   10.244.0.5   minikube   <none>           <none>
        frontend-deployment-b84cc7d45-tzqq6   1/1     Running   0          2m33s   10.244.0.4   minikube   <none>           <none>






------------------------------------------------------------------------------------------------------------------------



Configura√ß√µes do Servi√ßo



kubectl apply -f k8s/frontend-service.yaml




kubectl get services -o wide

    NAME               TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE   SELECTOR
    frontend-service   LoadBalancer   10.96.254.101   <pending>     8090:32699/TCP   39s   app=frontend






------------------------------------------------------------------------------------------------------------------------



Usar o Minikube Tunnel

O Minikube fornece uma ferramenta chamada minikube tunnel que pode ser usada para expor servi√ßos do tipo LoadBalancer.


    minikube tunnel


        [sudo] senha para wolf:
        Status:
                machine: minikube
                pid: 320451
                route: 10.96.0.0/12 -> 192.168.49.2
                minikube: Running
                services: [frontend-service]
            errors:
                        minikube: no errors
                        router: no errors
                        loadbalancer emulator: no errors




kubectl get services -o wide


      NAME               TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE     SELECTOR
      frontend-service   LoadBalancer   10.96.254.101   10.96.254.101   8090:32699/TCP   7m54s   app=frontend





http://10.96.254.101:8090/






curl -i 10.96.254.101:8090


      HTTP/1.1 200 OK
      Server: nginx/1.25.2
      Date: Sat, 29 Mar 2025 06:42:55 GMT
      Content-Type: text/html
      Content-Length: 482
      Last-Modified: Sat, 29 Mar 2025 02:13:49 GMT
      Connection: keep-alive
      ETag: "67e7575d-1e2"
      Accept-Ranges: bytes

      <!doctype html>
      <html lang="en" data-beasties-container>
      <head>
        <meta charset="utf-8">
        <title>Wolfstore</title>
        <base href="/">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="icon" type="image/x-icon" href="favicon.ico">
      <link rel="stylesheet" href="styles-5INURTSO.css"></head>
      <body>
        <app-root></app-root>
      <script src="polyfills-FFHMD2TL.js" type="module"></script><script src="main-KZ3DSSKO.js" type="module"></script></body>





------------------------------------------------------------------------------------------------------------------------







minikube start -p wolf-cluster --nodes=3


      üòÑ  [wolf-cluster] minikube v1.35.0 on Ubuntu 24.04
      ‚ú®  Automatically selected the docker driver
      üìå  Using Docker driver with root privileges

      üëç  Starting "wolf-cluster" primary control-plane node in "wolf-cluster" cluster
      üöú  Pulling base image v0.0.46 ...
      üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
      üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          ‚ñ™ Generating certificates and keys ...
          ‚ñ™ Booting up control plane ...
          ‚ñ™ Configuring RBAC rules ...
      üîó  Configuring CNI (Container Networking Interface) ...
      üîé  Verifying Kubernetes components...
          ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
      üåü  Enabled addons: storage-provisioner, default-storageclass

      üëç  Starting "wolf-cluster-m02" worker node in "wolf-cluster" cluster
      üöú  Pulling base image v0.0.46 ...
      üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
      üåê  Found network options:
          ‚ñ™ NO_PROXY=192.168.49.2
      üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          ‚ñ™ env NO_PROXY=192.168.49.2
      üîé  Verifying Kubernetes components...

      üëç  Starting "wolf-cluster-m03" worker node in "wolf-cluster" cluster
      üöú  Pulling base image v0.0.46 ...
      üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
      üåê  Found network options:
          ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
      üê≥  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
          ‚ñ™ env NO_PROXY=192.168.49.2
          ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
      üîé  Verifying Kubernetes components...
      üèÑ  Done! kubectl is now configured to use "wolf-cluster" cluster and "default" namespace by default











------------------------------------------------------------------------------------------------------------------------


1. Verificar o Status do Cluster
sh
minikube status -p wolf-cluster
2. Listar Todos os N√≥s no Cluster
sh
kubectl get nodes --context=wolf-cluster
3. Obter Informa√ß√µes Detalhadas de um N√≥
sh
kubectl describe node <node-name>
4. Verificar os Pods em Todos os Namespaces
sh
kubectl get pods --all-namespaces
5. Listar Todos os Servi√ßos em Todos os Namespaces
sh
kubectl get services --all-namespaces
6. Obter Informa√ß√µes Sobre o Contexto Atual
sh
kubectl config current-context
7. Obter Informa√ß√µes Sobre o Cluster
sh
kubectl cluster-info
8. Obter Informa√ß√µes Detalhadas de um Pod
sh
kubectl describe pod <pod-name> -n <namespace>
9. Verificar o Uso de Recursos de um N√≥
sh
kubectl top node
10. Verificar o Uso de Recursos de um Pod
sh
kubectl top pod -n <namespace>
Exemplos
Verificar o Status do Cluster:

sh
minikube status -p wolf-cluster
Listar Todos os N√≥s no Cluster:

sh
kubectl get nodes --context=minikube
Obter Informa√ß√µes Detalhadas de um N√≥:

sh
kubectl describe node minikube
Verificar os Pods em Todos os Namespaces:

sh
kubectl get pods --all-namespaces
Listar Todos os Servi√ßos em Todos os Namespaces:

sh
kubectl get services --all-namespaces
Obter Informa√ß√µes Sobre o Contexto Atual:

sh
kubectl config current-context
Obter Informa√ß√µes Sobre o Cluster:

sh
kubectl cluster-info
Obter Informa√ß√µes Detalhadas de um Pod:

sh
kubectl describe pod my-pod -n default
Verificar o Uso de Recursos de um N√≥:

sh
kubectl top node
Verificar o Uso de Recursos de um Pod:

sh
kubectl top pod -n default


------------------------------------------------------------------------------------------------------------------------


kubectl apply -f k8s/namespace.yaml


kubectl apply -f k8s/

      configmap/frontend-config created
      deployment.apps/frontend-deployment created
      secret/frontend-secrets created
      service/frontend-service created
      namespace/wolfns unchanged



kubectl config set-context --current --namespace=wolfns



------------------------------------------------------------------------------------------------------------------------




minikube tunnel -p wolf-cluster


        [sudo] senha para wolf:
        Status:
          machine: wolf-cluster
          pid: 988097
          route: 10.96.0.0/12 -> 192.168.49.2
          minikube: Running
          services: [frontend-service, frontend-service]
            errors:
            minikube: no errors
            router: no errors
            loadbalancer emulator: no errors




kubectl get svc


        NAME               TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)          AGE
        frontend-service   LoadBalancer   10.111.62.235   10.111.62.235   8090:30124/TCP   7m53s




------------------------------------------------------------------------------------------------------------------------



Instalar o Helm


# Instalar o Helm


curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash


          % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                         Dload  Upload   Total   Spent    Left  Speed
        100 11913  100 11913    0     0  36471      0 --:--:-- --:--:-- --:--:-- 36542
        Downloading https://get.helm.sh/helm-v3.17.2-linux-amd64.tar.gz
        Verifying checksum... Done.
        Preparing to install helm into /usr/local/bin
        [sudo] senha para wolf:
        helm installed into /usr/local/bin/helm



helm version

        version.BuildInfo{Version:"v3.17.2", GitCommit:"cc0bbbd6d6276b83880042c1ecb34087e84d41eb", GitTreeState:"clean", GoVersion:"go1.23.7"}



------------------------------------------------------------------------------------------------------------------------



Instalar o cert-manager


Vamos instalar o cert-manager para gerenciar certificados TLS com Let's Encrypt.


# Adicionar o reposit√≥rio Helm do cert-manager

helm repo add jetstack https://charts.jetstack.io

helm repo update

      Hang tight while we grab the latest from your chart repositories...
      ...Successfully got an update from the "jetstack" chart repository
      ...Successfully got an update from the "kubernetes-dashboard" chart repository
      ...Successfully got an update from the "prometheus-community" chart repository
      ...Successfully got an update from the "bitnami" chart repository



# Criar o namespace para o cert-manager

kubectl create namespace cert-manager



# Instalar o cert-manager usando o Helm

helm install cert-manager jetstack/cert-manager --namespace cert-manager --version v1.5.3



# Verificar a instala√ß√£o do cert-manager

kubectl get pods --namespace cert-manager


      NAME                                       READY   STATUS    RESTARTS      AGE
      cert-manager-78764f8796-bjfmm              1/1     Running   0             4m25s
      cert-manager-cainjector-7dd7ccf897-srqwh   1/1     Running   0             4m25s
      cert-manager-startupapicheck-szszg         1/1     Running   3 (49s ago)   4m24s
      cert-manager-webhook-68fddb647b-4zj5t      1/1     Running   0             4m25s





------------------------------------------------------------------------------------------------------------------------



Configurar o Issuer ou ClusterIssuer


Crie um arquivo letsencrypt-issuer.yaml para configurar o Issuer ou ClusterIssuer:


../wolf-cloud-native/cluster-kubernetes/letsencrypt-issuer.yaml


apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    server: https://acme-v02.api.letsencrypt.org/directory
    email: your-email@example.com
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
    - http01:
        ingress:
          class: nginx



kubectl apply -f letsencrypt-issuer.yaml


    kubectl apply -f letsencrypt-issuer.yaml
    error: resource mapping not found for name: "letsencrypt-prod" namespace: "" from "letsencrypt-issuer.yaml": no matches for kind "ClusterIssuer" in version "cert-manager.io/v1"
    ensure CRDs are installed first



kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v1.5.3/cert-manager.crds.yaml


      customresourcedefinition.apiextensions.k8s.io/certificaterequests.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/certificates.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/challenges.acme.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/clusterissuers.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/issuers.cert-manager.io created
      customresourcedefinition.apiextensions.k8s.io/orders.acme.cert-manager.io created




kubectl get crds | grep cert-manager


      certificaterequests.cert-manager.io   2025-03-30T04:46:18Z
      certificates.cert-manager.io          2025-03-30T04:46:18Z
      challenges.acme.cert-manager.io       2025-03-30T04:46:18Z
      clusterissuers.cert-manager.io        2025-03-30T04:46:18Z
      issuers.cert-manager.io               2025-03-30T04:46:18Z
      orders.acme.cert-manager.io           2025-03-30T04:46:19Z




kubectl apply -f letsencrypt-issuer.yaml


      clusterissuer.cert-manager.io/letsencrypt-prod created








------------------------------------------------------------------------------------------------------------------------



5. Instalar o NGINX Kubernetes Gateway


# Adicionar o reposit√≥rio Helm do NGINX

helm repo add nginx-stable https://helm.nginx.com/stable

      "nginx-stable" has been added to your repositories



helm repo update

      Hang tight while we grab the latest from your chart repositories...
      ...Successfully got an update from the "nginx-stable" chart repository
      ...Successfully got an update from the "kubernetes-dashboard" chart repository
      ...Successfully got an update from the "jetstack" chart repository
      ...Successfully got an update from the "prometheus-community" chart repository
      ...Successfully got an update from the "bitnami" chart repository
      Update Complete. ‚éàHappy Helming!‚éà



# Instalar o NGINX Kubernetes Gateway

helm install nginx-gateway nginx-stable/nginx-kubernetes-gateway

      helm repo update
      Hang tight while we grab the latest from your chart repositories...
      ...Successfully got an update from the "kubernetes-dashboard" chart repository
      ...Successfully got an update from the "nginx-stable" chart repository
      ...Successfully got an update from the "jetstack" chart repository
      ...Successfully got an update from the "prometheus-community" chart repository
      ...Successfully got an update from the "bitnami" chart repository
      Update Complete. ‚éàHappy Helming!‚éà
      wolf@dell-Inspiron:$ helm install nginx-gateway nginx-stable/nginx-kubernetes-gateway
      Error: INSTALLATION FAILED: chart "nginx-kubernetes-gateway" matching  not found in nginx-stable index. (try 'helm repo update'): no chart name found

      O erro indica que o gr√°fico nginx-kubernetes-gateway n√£o foi encontrado no reposit√≥rio nginx-stable. Isso pode ocorrer se o nome do gr√°fico estiver incorreto ou se ele n√£o estiver dispon√≠vel no reposit√≥rio.






helm search repo nginx-stable

      NAME                                        	CHART VERSION	APP VERSION	DESCRIPTION
      nginx-stable/nginx-appprotect-dos-arbitrator	0.1.0        	1.1.0      	NGINX App Protect Dos arbitrator
      nginx-stable/nginx-devportal                	1.7.2        	1.7.2      	A Helm chart for deploying ACM Developer Portal
      nginx-stable/nginx-ingress                  	2.0.1        	4.0.1      	NGINX Ingress Controller
      nginx-stable/nginx-service-mesh             	2.0.0        	           	NGINX Service Mesh
      nginx-stable/nms                            	1.16.0       	NIM 2.19.0 	A chart for installing the NGINX Management Suite
      nginx-stable/nms-acm                        	1.9.3        	1.9.3      	A Helm chart for Kubernetes
      nginx-stable/nms-adm                        	4.0.0        	4.0.0      	A Helm chart for ADM
      nginx-stable/nms-hybrid                     	2.19.1       	2.19.1     	A Helm chart for Kubernetes



helm install nginx-gateway nginx-stable/nginx-ingress


      NAME: nginx-gateway
      LAST DEPLOYED: Sun Mar 30 01:55:56 2025
      NAMESPACE: wolfns
      STATUS: deployed
      REVISION: 1
      TEST SUITE: None
      NOTES:
      NGINX Ingress Controller 4.0.1 has been installed.

      For release notes for this version please see: https://docs.nginx.com/nginx-ingress-controller/releases/

      Installation and upgrade instructions: https://docs.nginx.com/nginx-ingress-controller/installation/installing-nic/installation-with-helm/








------------------------------------------------------------------------------------------------------------------------



6. Configurar o Ingress para Usar o Certificado


Crie um arquivo ingress.yaml para configurar o Ingress para usar o certificado Let's Encrypt:




apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"  # Use "issuer" se voc√™ estiver usando Issuer
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
  - hosts:
    - wolfstore.com
    secretName: example-tls
  rules:
  - host: wolfstore.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: my-service
            port:
              number: 80



kubectl apply -f ingress.yaml

    ingress.networking.k8s.io/example-ingress created











------------------------------------------------------------------------------------------------------------------------




7. Configurar Network Policies


Crie um arquivo network-policy.yaml para definir regras de rede:



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: backend
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          role: backend
    ports:
    - protocol: TCP
      port: 80






kubectl apply -f network-policy.yaml




------------------------------------------------------------------------------------------------------------------------




Configurar RBAC


Crie um arquivo rbac.yaml para gerenciar permiss√µes:





apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: "jane"
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io




kubectl apply -f rbac.yaml

    role.rbac.authorization.k8s.io/pod-reader created
    rolebinding.rbac.authorization.k8s.io/read-pods created





------------------------------------------------------------------------------------------------------------------------



9. Configurar Pod Security Policies


Crie um arquivo pod-security-policy.yaml para aplicar restri√ß√µes aos pods:




apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  allowedCapabilities: []
  volumes:
  - 'configMap'
  - 'emptyDir'
  - 'projected'
  - 'secret'
  - 'downwardAPI'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535




kubectl apply -f pod-security-policy.yaml


    error: resource mapping not found for name: "restricted-psp" namespace: "" from "pod-security-policy.yaml": no matches for kind "PodSecurityPolicy" in version "policy/v1"
    ensure CRDs are installed first
    wolf@dell-Inspiron:$





------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------






------------------------------------------------------------------------------------------------------------------------






------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------





------------------------------------------------------------------------------------------------------------------------




------------------------------------------------------------------------------------------------------------------------






